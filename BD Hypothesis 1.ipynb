{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAPzv8u0gK1C"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWWyXsY9c2H5"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyS7UWfuZeSR"
   },
   "outputs": [],
   "source": [
    "# see what already avail and thus determine which steps required prior to reading in file and handling the data\n",
    "# if you see more than \"sample_data\" you can jump to the relevant step below\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ZMzK1NEOxG"
   },
   "outputs": [],
   "source": [
    "# set-up spark (NB if Apache amend versions on download site we will need to amend path in wget command)\n",
    "print(\"\\nWelcome to advanced top sites\")\n",
    "!ls\n",
    "!rm -f spark-3.3.[01]-bin-hadoop3.tgz* \n",
    "!rm -rf spark-3.3.[01]-bin-hadoop3\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "!tar -xf spark-3.3.2-bin-hadoop3.tgz\n",
    "!ls -alt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7ep6be3O27B"
   },
   "outputs": [],
   "source": [
    "# install findspark if not already installed\n",
    "!pip3 install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZguLonUE-js"
   },
   "outputs": [],
   "source": [
    "# init spark (ensure SPARK_HOME set to same version as we download earlier)\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\n",
    "spark_conf = SparkConf().setMaster('local[2]').setAppName('MyApp')\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "# see what we have by examining the Spark User Interface\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "# \"SparkSession\" and \"sc\" are are key handles in to Spark API\n",
    "##SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHbddaLQUNwo"
   },
   "outputs": [],
   "source": [
    "# get file for given year from TfL open data\n",
    "!wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
    "!unzip cyclehireusagestats-2014.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSHwoU3Ra_BR"
   },
   "source": [
    "# First Hypothesis\n",
    "\n",
    " “In 2014, people ride for longer in Autumn than in Spring” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aepYin6bXWB"
   },
   "source": [
    "Autumn Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7174xYslVvm"
   },
   "outputs": [],
   "source": [
    "# MULTIPLE FILES INPUT AND ANALYSED (presuming identical schema - should test first!)\n",
    "file1=\"./10a. Journey*csv\" # 14 Sep to 27 Sep\n",
    "file2=\"./10b. Journey*csv\" # 28 Sep to 11 Oct\n",
    "file3=\"./11a. Journey*csv\" # 12 Oct to 08 Nov\n",
    "file4=\"./12b. Journey*csv\" # 09 Nov to 06 Dec\n",
    "file5=\"./13a. Journey*csv\" # 07 Dec to 21 Dec\n",
    "# we might say therefore that data in these 5 files corresponds to \"autumn\" in the UK\n",
    "autumn_df = (spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .load([file1, file2, file3, file4, file5])) # i.e pass a Python list of files to load (into a single DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nVdbevXW1EY"
   },
   "outputs": [],
   "source": [
    "autumn_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcL9sTyz8h77"
   },
   "outputs": [],
   "source": [
    "autumn_df = autumn_df.withColumn(\"Duration\", col(\"Duration\").cast(\"float\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atRN0VN5wG3L"
   },
   "outputs": [],
   "source": [
    "autumn_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pCLSRoOv5QY"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bs2fC7os0OFK"
   },
   "source": [
    "Removing the rows that are not in autumn season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE6JiTrIt4AI"
   },
   "outputs": [],
   "source": [
    "# Specify the date columns to be converted to timestamp\n",
    "date_columns = [\"End Date\", \"Start Date\"]\n",
    "\n",
    "# Convert the date columns to timestamp format\n",
    "for column in date_columns:\n",
    "    autumn_df = autumn_df.withColumn(column, to_timestamp(col(column), \"dd/MM/yyyy HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laKMvVcfcIv8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the filter date string to timestamp\n",
    "filter_date = \"23/09/2014 00:00\"\n",
    "filter_timestamp = pd.to_datetime(filter_date, format=\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "# Filter the DataFrame to remove rows before the specified date\n",
    "autumn_df = autumn_df.filter(col(\"Start Date\") >= filter_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXKF5znVrAGw"
   },
   "outputs": [],
   "source": [
    "# TO DO: how many rows do we have in the autumn_df DF?\n",
    "autumn_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5JM4rJ90Xob"
   },
   "source": [
    "Inspecting for the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_5kbEcsyRa4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = autumn_df.agg(*[\n",
    "    sum(col(column).isNull().cast(\"integer\")).alias(column)\n",
    "    for column in autumn_df.columns\n",
    "])\n",
    "\n",
    "# Display the count of missing values in each column\n",
    "missing_values.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85rGVLmO0fDJ"
   },
   "source": [
    "Inspecting and removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7Sq36_0y5fF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the column to check for outliers\n",
    "column_name = \"Duration\"\n",
    "\n",
    "# Extract the column as a list\n",
    "autumn_durations = autumn_df.select(column_name).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create a box plot\n",
    "plt.boxplot(autumn_durations)\n",
    "plt.xlabel(column_name)\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Box Plot - \" + column_name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tp2i5hLpzZx2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to select rows with duration below 0\n",
    "negative_duration_df = autumn_df.filter(col(\"Duration\") < 0)\n",
    "\n",
    "# Show the resulting rows\n",
    "negative_duration_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54RJuM7az3oT"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to remove rows with duration below 0\n",
    "autumn_df = autumn_df.filter(col(\"Duration\") >= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jODOAxL1xlcI"
   },
   "outputs": [],
   "source": [
    "summary_stats = autumn_df.select(\"Duration\").describe()\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j26e6S18-vhk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OUo1ATP-wez"
   },
   "source": [
    "Spring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ryCxDRr-wfH"
   },
   "outputs": [],
   "source": [
    "# MULTIPLE FILES INPUT AND ANALYSED (presuming identical schema - should test first!)\n",
    "file1=\"./3. Journey*csv\" # 02 Mar to 31 Mar\n",
    "file2=\"./4. Journey*csv\" # 01 Apr to 26 Apr\n",
    "file3=\"./5. Journey*csv\" # 27 Apr to 24 May\n",
    "file4=\"./6. Journey*csv\" # 25 May to 21 Jun\n",
    "# we might say therefore that data in these 5 files corresponds to \"autumn\" in the UK\n",
    "spring_df = (spark.read.format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .load([file1, file2, file3, file4])) # i.e pass a Python list of files to load (into a single DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlkHSiSq-wfI"
   },
   "outputs": [],
   "source": [
    "spring_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TObvVPLx-wfI"
   },
   "outputs": [],
   "source": [
    "spring_df = spring_df.withColumn(\"Duration\", col(\"Duration\").cast(\"float\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7rrYTKOE0SA"
   },
   "outputs": [],
   "source": [
    "# Specify the date columns to be converted to timestamp\n",
    "date_columns = [\"End Date\", \"Start Date\"]\n",
    "\n",
    "# Convert the date columns to timestamp format\n",
    "for column in date_columns:\n",
    "    spring_df = spring_df.withColumn(column, to_timestamp(col(column), \"dd/MM/yyyy HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEMMB4c7-wfJ"
   },
   "outputs": [],
   "source": [
    "spring_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZSX7SUo-wfJ"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYX-YYxx-wfJ"
   },
   "source": [
    "Removing the rows that are not in spring season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ueil3Vvg-wfK"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the filter date string to timestamp\n",
    "filter_date = \"20/03/2014 00:00\"\n",
    "filter_timestamp = pd.to_datetime(filter_date, format=\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "# Filter the DataFrame to remove rows before the specified date\n",
    "spring_df = spring_df.filter(col(\"Start Date\") >= filter_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBVQ8b0l-wfK"
   },
   "outputs": [],
   "source": [
    "# TO DO: how many rows do we have in the autumn_df DF?\n",
    "spring_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E10MjC45-wfL"
   },
   "source": [
    "Inspecting for the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAP372mK-wfL"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = spring_df.agg(*[\n",
    "    sum(col(column).isNull().cast(\"integer\")).alias(column)\n",
    "    for column in spring_df.columns\n",
    "])\n",
    "\n",
    "# Display the count of missing values in each column\n",
    "missing_values.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScV5OPvQ-wfL"
   },
   "source": [
    "Inspecting and removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9VupIhC-wfM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the column to check for outliers\n",
    "column_name = \"Duration\"\n",
    "\n",
    "# Extract the column as a list\n",
    "spring_durations = spring_df.select(column_name).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create a box plot\n",
    "plt.boxplot(spring_durations)\n",
    "plt.xlabel(column_name)\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Box Plot - \" + column_name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXAIhbT8-wfM"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to select rows with duration below 0\n",
    "negative_duration_df = spring_df.filter(col(\"Duration\") < 0)\n",
    "\n",
    "# Show the resulting rows\n",
    "negative_duration_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgkgoDZs-wfN"
   },
   "outputs": [],
   "source": [
    "summary_stats = spring_df.select(\"Duration\").describe()\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phx-ErYC_fw2"
   },
   "outputs": [],
   "source": [
    "seasons = ['Autumn', 'Spring']\n",
    "autumn_avg_duration = autumn_df.agg({'Duration': 'mean'}).first()[0]\n",
    "spring_avg_duration = spring_df.agg({'Duration': 'mean'}).first()[0]\n",
    "ride_durations = [autumn_avg_duration, spring_avg_duration]\n",
    "\n",
    "plt.bar(seasons, ride_durations)\n",
    "plt.xlabel('Seasons')\n",
    "plt.ylabel('Average Ride Duration')\n",
    "plt.title('Comparison of Average Ride Durations in Autumn and Spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMOYZfs5CxXl"
   },
   "outputs": [],
   "source": [
    "seasons = ['Autumn', 'Spring']\n",
    "ride_durations = [autumn_durations, spring_durations]  \n",
    "plt.boxplot(ride_durations, labels=seasons)\n",
    "plt.xlabel('Seasons')\n",
    "plt.ylabel('Ride Duration')\n",
    "plt.title('Comparison of Ride Durations in Autumn and Spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QolvwwM_dfl"
   },
   "source": [
    "Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rcs73kWG6YS"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "t_statistic, p_value = ttest_ind(autumn_durations, spring_durations, equal_var=True)\n",
    "\n",
    "# Print the test result\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQdCCIWBKI9Q"
   },
   "source": [
    "T-Statistic: The t-statistic measures the difference between the means of the two groups (Autumn and Spring) relative to the variation within each group. In this case, the calculated t-statistic is approximately -27.74. The negative sign indicates that the average ride durations in Autumn are significantly shorter than those in Spring.\n",
    "\n",
    "P-Value: The p-value represents the probability of observing a t-statistic as extreme as the one calculated, assuming that there is no difference between the groups (null hypothesis). In this case, the p-value is approximately 2.345e-169, which is extremely close to zero. A p-value this small suggests strong evidence against the null hypothesis and indicates that the observed difference in ride durations is highly unlikely to occur by chance alone.\n",
    "\n",
    "Based on these results, we can draw the following conclusions:\n",
    "\n",
    "The average ride durations in Autumn and Spring are significantly different.\n",
    "The negative t-statistic suggests that the average ride durations in Autumn are significantly shorter than those in Spring.\n",
    "The extremely small p-value provides strong evidence to reject the null hypothesis and supports the conclusion of a significant difference between Autumn and Spring.\n",
    "These findings indicate that the season has an impact on the average ride durations, with Autumn and Spring exhibiting distinct patterns. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AAPzv8u0gK1C"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
